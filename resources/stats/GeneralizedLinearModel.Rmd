---
title: 'Advanced Methods: GLM'
output:
github_document:
toc: yes
toc_float: yes
---

```{r}
source('StatsResourcesConfig.R')
```

In the (previous notebook)[https://zenkavi.github.io/training/html/CorrelationTTestAnova.nb.html] we looked at simple and multiple linear models. Here we'll briefly cover types of data where this method could not be used and ways of dealing with it.

## Generalized linear models

- Number of variables: >=2  
- Type of variables: 1 DV of any type that does not have a normally distributed error distribution (e.g. binary responses, count data), >=1 IV both continuos and categorical
- Output: (standardized) regression coefficients, t value, df, p value  
- Typical plot: scatterplot, boxplot, barplot  

### Terminology

First a quick note on clarifying the different terms you might hear involving the term 'regression':  
(Simple) Linear regression (1 IV) *<* General linear regression (= multivariate regression model (>= 1 DVs) != multinomial regression; e.g. simple or multiple linear (>= 1 IV) regression (1 DV), t-test, ANOVA) *<* Generalized linear regression (e.g. logit, Poisson, probit, multinomial etc.)

Here's how they would look if we write them out in `R`

*Simple linear regression*

```{r eval=FALSE}
lm(DV ~ IV, data)
```

*Multiple linear regression*

```{r eval=FALSE}
lm(DV ~ IV_1+IV_2, data)
```

*Multivariate linear regression*

```{r eval=FALSE}
lm(cbind(DV_1, DV_2) ~ IV_1+IV_2, data)
```

*Logit regression*

```{r eval=FALSE}
glm(DV ~ IV_1+IV_2, data, family =binomial(link = "logit"))
```

*Poisson regression* 

```{r eval=FALSE}
glm(DV ~ IV_1+IV_2, data, family =poisson(link = "log"))
```

### Assumptions of linear regression

- Normality of residuals  
- Heteroscedasticity: variance should not increase or decrease with mean of values  

When these are not satisfied using `lm` could lead to confusing/wrong conclusions! 

How do we evaluate an `lm`?

### Interpreting and evaluating a model

The idea behind a (simple) linear regression is one familiar from high-school match. We interpret the output of the model as the intercept and slope of a line.

In other words one unit increase in `x` leads to an `m` unit increase in `y` and `c` is the value of `y` for `x=0`.

```{r eval=FALSE}
y = m*x+c
```

This linear relation/interpretation will be the continuing theme for different types regressions too BUT the form of `y` and thus the interpretation of `m` will change!  

How do we decide if a model is good in describing the data?  

Last time we talked about a somewhat related question: how do we decide which model is better in describing the data if we have several models? We talked about the `anova` function. This was to compare a very specific/limited set of models with very specific set of structure (nested).

A more general approach would be to compare what the model predicts to the actual data and quantify the amount of deviation. We will make use of this approach in detecting the problems with models that do not apply to specific types of data.

Side note: There are also certain statistics that quantify model fit: $R^2$, AIC, BIC, cross-validated accuracy etc. We'll talk more about these another time.

Side note: Remember the point of a model is to describe a certain pattern in the data accurately. Data is what you should believe in, models can be wrong and need revision! (An example where this principle could be applied: is a hyperbolic model always the best way to describe discounting behavior? Could there be more heuristic ways in certain situations? Could it be domain dependent? Before just fitting the model for every subject you should evaluate whether the model does a good job in capturing the behavior. There is no universal law that says humans must discount hyperbolically!)

#### Interpretive problems for variables with non-normal distributions

What might happen if you don't use the right kind of model to describe a relationship in your data?  
- Not capturing the effect of a variable equally well across the whole distribution of the DV.  
- Predicting values outside the range of meaningful values.  
- Statements on units that do not translate to meaningful changes.  

### Data Examples

Let's remember one model we ran for the `iris` dataset last time.

```{r}
iris
```

We'll look at the relationship between `Petal.Width` and `Petal.Length`

```{r}
iris %>% 
  ggplot(aes(Petal.Length, Petal.Width, col=Species))+
  geom_point()
```

```{r}
m1_lm = lm(Petal.Width ~ Petal.Length+Species, iris)
```

What does this model suggest?

```{r}
summary(m1_lm)
```

### Predicted vs. Actual Plot

```{r}
iris %>%
  mutate(pred_pw = predict(m1_lm)) %>%
  ggplot(aes(Petal.Width, pred_pw))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")
```

Let's look another dataset: `mtcars`

```{r}
mtcars
```

Structure of the data

```{r}
str(mtcars)
```

Relationship between `vs` (type of motor) and `mpg`

What type of variable is `vs`? What is different about it compared to `mpg`?

```{r}
mtcars %>%
  ggplot(aes(factor(vs), mpg))+
  geom_boxplot()+
  coord_flip()
```

If you want to described the relationship between `vs` and `mpg` how would you formulate the question?  

What is the interpretation of the following model?

```{r}
m2_lm = lm(vs ~ mpg, mtcars)
```

```{r}
summary(m2_lm)
```

Let's look at what this model does visually

```{r}
mtcars %>%
  ggplot(aes(mpg, vs))+
  geom_point()+
  geom_smooth(method="lm")+
  scale_y_continuous(breaks = c(0,1))
```

What is the problem with the predictions of thi model/figure?

```{r}
mtcars %>%
  mutate(pred_vs = predict(m2_lm)) %>%
  ggplot(aes(factor(vs), pred_vs))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")+
  geom_hline(yintercept=0, col="red")+
  geom_hline(yintercept=1, col="red")
```

What would we have preffed?

```{r}
mtcars %>%
  ggplot(aes(mpg, vs))+
  geom_point()+
  geom_smooth(method = "glm", method.args = list(family = "binomial"))+
  scale_y_continuous(breaks = c(0,1))
```

#### Link functions

The link function provides the relationship between the linear predictor and the mean of the distribution function

### Linking function versus transformation

An log transform RT example

Another relevant example: discount rates

### GLM with binomial link function

What is this model doing? 

**Predicting the log odds of `vs` instead of `vs` using `mpg`**

```{r}
m2_glm = glm(vs ~ mpg, mtcars, family = binomial(link = "logit"))
```

How do we interpret the output?

```{r}
summary(m2_glm)
```

What do the predictions of this model look like?

```{r}
mtcars %>%
  mutate(pred_vs = exp(predict(m2_glm))/(1+exp(predict(m2_glm)))) %>%
  ggplot(aes(factor(vs), pred_vs))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")
```

### GLM with Poisson link function

What kinds of limitations would a distribution of counts have?

```{r}
mtcars %>%
  ggplot(aes(wt, gear))+
  geom_point()
```

Note: Poisson model's assume: mean = variance. This doesn't really hold for this data. There are other types of link functions that can be used too but I won't not cover them for now.

```{r}
mtcars %>%
  group_by(gear) %>%
  summarise(var_wt = var(wt),
            mean_wt = mean(wt))
```

If we were to run a simple linear model how would we interpret that?

```{r}
m3_lm = lm(gear ~ wt, mtcars)
```

```{r}
summary(m3_lm)
```

This is what the linear model does visually:

```{r}
mtcars %>%
  ggplot(aes(wt, gear))+
  geom_point()+
  geom_smooth(method = "lm")
```

Here are the predictions of the simple linear model. What are some problems?

```{r}
mtcars %>%
  mutate(pred_gear = predict(m3_lm)) %>%
  ggplot(aes(factor(gear), pred_gear))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")+
  scale_y_continuous(limits = c(2,5))+
  geom_hline(yintercept=5, color="red")+
  geom_hline(yintercept=4, color="red")+
  geom_hline(yintercept=3, color="red")

```

What is this model doing?

**Predicting the log-linear of `gear` using `wt` instead of the raw `gear` values**

```{r}
m3_glm = glm(gear ~ wt, mtcars, family = poisson(link = "log"))
```

```{r}
summary(m3_glm)
```

```{r}
mtcars %>%
  ggplot(aes(wt, gear))+
  geom_point()+
  geom_smooth(method = "glm", method.args = list(family = "poisson"), fill="blue", alpha=0.2)+
  geom_smooth(method = "lm", color="red", fill="red", alpha = 0.2)
```

```{r}
mtcars %>%
  ggplot(aes(wt, log(gear)))+
  geom_point()+
  geom_smooth(method = "lm")
```

```{r}
mtcars %>%
  mutate(pred_gear = exp(predict(m3_glm))) %>%
  ggplot(aes(gear, pred_gear))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")
```


