---
title: 'Advanced Methods: GLM'
output:
github_document:
toc: yes
toc_float: yes
---

```{r}
source('StatsResourcesConfig.R')
```

In the (previous notebook)[https://zenkavi.github.io/training/html/CorrelationTTestAnova.nb.html] we looked at linear models. In those models the residauals wwere assumed to be independent and normally distributed. 

## Generalized linear models

- Number of variables: >=2  
- Type of variables: 1 DV of any type that does not have a normally distributed error distribution (e.g. binary responses, count data), >=1 IV both continuos and categorical
- Output: (standardized) regression coefficients, t value, df, p value  
- Typical plot: scatterplot, boxplot, barplot  

### Terminology

General linear model
Generalized linear model
Multiple regression
Multivariate regression

### Assumptions of linear regression

Normality of residuals
Heteroscasdicity (check spelling): variance should not increase or decrease with mean of values

### Linking function versus transformation

An log transform RT example

Another relevant example: discount rates

### Interpreting and evaluating a model

Last time we talked about the `anova` function. This was to compare very specific (limited) set of models with very specific set of structure (nested).

A more general way would be to compare what the model predicts to the actual data and quantify the amount of deviation.

There are also certain statistics that quantify model fit: $R^2$, AIC, BIC, cross-validated accuracy etc.

#### Interpretive problems for variables with non-normal distributions

```{r}
iris
```


```{r}
iris %>% 
  ggplot(aes(Petal.Length, Petal.Width, col=Species))+
  geom_point()
```

```{r}
m1_lm = lm(Petal.Width ~ Petal.Length+Species, iris)
```

```{r}
summary(m1_lm)
```


```{r}
iris %>%
  mutate(pred_pw = predict(m1_lm)) %>%
  ggplot(aes(Petal.Width, pred_pw))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")
```

#### Link functions

### GLM with binomial link function

Let's look another dataset: `mtcars`

```{r}
mtcars
```

Structure of the data

```{r}
str(mtcars)
```

Relationship between `vs` (type of motor) and `mpg`

What type of variable is `vs`? What is different about it compared to `mpg`?

```{r}
mtcars %>%
  ggplot(aes(factor(vs), mpg))+
  geom_boxplot()+
  coord_flip()
```

If you want to described the relationship between `vs` and `mpg` how would you formulate the question?  

What is the interpretation of the following model?

```{r}
m2_lm = lm(vs ~ mpg, mtcars)
```

```{r}
summary(m2_lm)
```

Let's look at what this model does visually

```{r}
mtcars %>%
  ggplot(aes(mpg, vs))+
  geom_point()+
  geom_smooth(method="lm")+
  scale_y_continuous(breaks = c(0,1))
```

What is the problem with the predictions of thi model/figure?

```{r}
mtcars %>%
  mutate(pred_vs = predict(m2_lm)) %>%
  ggplot(aes(factor(vs), pred_vs))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")+
  geom_hline(yintercept=0, col="red")+
  geom_hline(yintercept=1, col="red")
```

What would we have preffed?

```{r}
mtcars %>%
  ggplot(aes(mpg, vs))+
  geom_point()+
  geom_smooth(method = "glm", method.args = list(family = "binomial"))+
  scale_y_continuous(breaks = c(0,1))
```

What is this model doing? 

**Predicting the log odds of `vs` instead of `vs` using `mpg`**

```{r}
m2_glm = glm(vs ~ mpg, mtcars, family = binomial(link = "logit"))
```

How do we interpret the output?

```{r}
summary(m2_glm)
```

What do the predictions of this model look like?

```{r}
mtcars %>%
  mutate(pred_vs = exp(predict(m2_glm))/(1+exp(predict(m2_glm)))) %>%
  ggplot(aes(factor(vs), pred_vs))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")
```

### GLM with Poisson link function

What kinds of limitations would a distribution of counts have?

```{r}
mtcars %>%
  ggplot(aes(wt, gear))+
  geom_point()
```

Note: Poisson model's assume: mean = variance. This doesn't really hold for this data. There are other types of link functions that can be used too but I won't not cover them for now.

```{r}
mtcars %>%
  group_by(gear) %>%
  summarise(var_wt = var(wt),
            mean_wt = mean(wt))
```

If we were to run a simple linear model how would we interpret that?

```{r}
m3_lm = lm(gear ~ wt, mtcars)
```

```{r}
summary(m3_lm)
```

This is what the linear model does visually:

```{r}
mtcars %>%
  ggplot(aes(wt, gear))+
  geom_point()+
  geom_smooth(method = "lm")
```

Here are the predictions of the simple linear model. What are some problems?

```{r}
mtcars %>%
  mutate(pred_gear = predict(m3_lm)) %>%
  ggplot(aes(factor(gear), pred_gear))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")+
  scale_y_continuous(limits = c(2,5))+
  geom_hline(yintercept=5, color="red")+
  geom_hline(yintercept=4, color="red")+
  geom_hline(yintercept=3, color="red")

```

What is this model doing?

**Predicting the log-linear of `gear` using `wt` instead of the raw `gear` values**

```{r}
m3_glm = glm(gear ~ wt, mtcars, family = poisson(link = "log"))
```

```{r}
summary(m3_glm)
```

```{r}
## FIX THISS
mtcars %>%
  ggplot(aes(gear, wt))+
  geom_point()+
  geom_smooth(method = "glm", method.args = list(family = "log"))
```

```{r}
mtcars %>%
  mutate(pred_gear = exp(predict(m3_glm))) %>%
  ggplot(aes(gear, pred_gear))+
  geom_point()+
  xlab("Actual")+
  ylab("Predicted")
```



Remember: the point of a model is to describe a certain pattern in the data accurately. Data is what you should believe in, models can be wrong and need revision! (An example where this principle could be applied: is a hyperbolic model always the best way to describe discounting behavior? Could there be more heuristic ways in certain situations? Could it be domain dependent? Before just fitting the model for every subject you should evaluate whether the model does a good job in capturing the behavior. There is no universal law that says humans must discount hyperbolically!)
